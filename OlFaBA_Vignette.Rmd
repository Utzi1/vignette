---
title: "Vignette zum Paket OlfaBA"
author: "ULR"
date: "`r Sys.Date()`"
  fontfamily: arev
output:
  pdf_document: default
  html_document: default
  bibliography: /home/bachelor/Bachelor/Bibliographie/Bibliogrphie_BA_1.bib
vignette: |
  %\VignetteIndexEntry{Vignette Title} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
setwd("/home/bachelor/Bachelor/Paket_BHT/BTBA1")
library(tidyverse)
library(devtools)
library(pander)
```
# Das Paket BTBA1

Diese Paket soll als Langzeitalternative die Studenten im Studiengang Biotechnologie vor der Verwendung von Office-Programmen lösen.
Laden könnt ihr das Paket über:
```{r}
load_all(path = "~/Bachelor/Paket_BHT/BTBA1/")
```

# Grundlegendes

## Was macht R

Ich stelle mir R als einen mächtigen Taschenrechner vor, so mächtig, dass er wohl (fast) alles kann, warum? :

* R kann ableiten
* R kann aufleiten
* R kann mit komplexen Klassen arbeiten
* R kann Texte formatieren
* R ist beliebig erweiterbar
* eben wie ein virtueller, programmierbarer und schneller Taschenrechner!

Allerdings hängt es vom Anwender ab, wie gut all diese Möglichkeiten genutzt und angewandt werden.
Um das zu können, sollte man die Sprache R können, also mit dem Programm R interagieren.
Ein einfaches Beispiel:
```{r}
1 + 1 # das ist ein Kommentar (alles nach dem #)
1 + 2 # das sollte selbstverständlich sein
4 / 2 # auch das sollte selbstverständlich sei
2 ^ 2 # zwei im quadrat
pi    # das hier ist pi 
cos( pi ) # die trigonometrische Funktion cos von pi
sin( pi ) # die trigonometrische Funktion sin von pi
e  <- exp(1) # hier wird e zur eulerschen Zahl
print( e ) # print() kann uns die Variable e printen 
log( e ) # was ist der ln von e?
```
Am Beispiel von log() sehen wir, dass *log* etwas macht, diese Information was es verarbeitet ist dann in den Klammern *()* hinterlegt.
\texttt{log()} ist eine Funktion, eine Funktion um den natürlichen Logarithmus einer Zahl zu berechnen.
Funktionen wie log() sind das Rüstzeug um komplexeren Rechnungen zu trotzen, sie sind ein essentieller Teil und werden später genauer betrachtet.
Wir können beliebige Objekete erschaffen und diese Werte zuordnen um damit zu rechnen.
Beispielhaft: 

```{r}
    # einen Vektor
Vektor.1  <- c( 1, 2, 3, 4 ) # das c macht aus dem eingeklammerten
                             # eine Vektor 
Vektor.2  <- c( 5, 6, 7, 8 )

Vektor.3  <- Vektor.1 * Vektor.1 # man kann viel mit Vektoren anstellen
    # ein Tibble ist eine Tabelle
Vektoren <- tibble (    
                    Vektor.1,
                    Vektor.2,
                    Vektor.3
                    )
    # eine Liste bietet Platz für viel Information:
Liste  <- list( "Buchstaben" = c('a', 'b', 'c'),
                "Zahlen" = c(1, 2, 3), 
                "Ein Satz" = "Ick kann janich so viel fressen, wie ick kotzen möchte!
                , Max Liebermann")
    # Wenn wir nur den Namen eines Objekt schreiben
    # bekommen wir das Objekt ausgegeben 
Vektor.1
Vektor.2
Vektor.3
Vektoren
Liste
```

Eine besondere Wichtigkeit bekommt das Pfeilchen zugesprochen, es ist innerhalb der Sprache R in der Lage einen Wert zuzuordnen
Diese Zuordnung beschränkt nicht nur auf einfache Werte sondern gilt auch für Funktionen, Vektoren, Listen und so weiter.
Vektoren, Listen, Tibbles usw. werden als Klasse bezeichnet, durch Zuordnung von Werten werden aus den Klassen Objekten.
Man kann sich eine Klasse also ein wenig wie das Fließschema eines Bioreaktor vorstellen, wenn das Fließschema aus Metall gebaut wird und nicht mehr nur theoretisch existiert wird es zum Objekt, "mit Leben gefüllt", so wie wenn eben einer Klasse Werte zugeordnet werden.
Der Vektor entsteht also durch Zuordnung von Daten zu einer leeren Klasse und bekommt dann einen Namen und einen klaren Sinn.
Bei der Benennung eines Objektes sollte man sich Gedanken machen, nicht alle Namen sind sinnvoll, ein Leerzeichen kann da schon zu viel Problemen führen.
Möchte man ein Objekt benennen sollte man vermeiden:

* *-* ist eben ein Minus-Operator
* *+* ist der Plus-Operator
* _*_ ist der Mal-Operator
* */* ist ist zum teilen
* *?* beschafft Hilfe :-), probier aus! 
* *:* ein Sequenz-Operator
* *= , <, >, & * weitere Operatoren, später mehr!

Punkte und Unterstriche sind innerhalb der R-Umgebung ideal um Namen von Objekten zu strukturieren, Beispiele gab's bereits.
Mehr dazu findet man zum Beispiel in @R_gen oder @RFDS.
Um Vektoren durch bekannte mathematische Operationen zu generieren stehen uns verschiedene Funktionen zur Verfügung: 
```{r}
# von eins bis 10 zählen:
x  <- 1:10 %>% # eine Pipe, eine 'Röhre' zwischen den Funktinen
    print()
# Von 1 bis 10 in 0.5-er Schritten:
x  <- seq(1, 10, 0.5) %>%
    print()
# einen Vektor der Länge 5 generieren:
x <- seq(1, 10, len = 5) %>%
    print()
```
seq() eignet sich gut für Sequenzen, rep() ist der Counterpart und wiederholt.
Es ist auch möglich mit Buchstaben zu arbeiten, die heißen dann "character" aber interessieren uns erst mal nicht.
Wichtig zu beachten ist, ist dass es einen Unterschied macht, wenn wir einen Buchstaben ohne Anführungszeichen verwenden oder mit!
```{r}
print("Albatross")

Albatross <- 'Legehenne'

print(Albatross)

Satz <- "Die Legehenne kennt's Eierlegen gut, sie macht's die ganze Zeit"
print(Satz)
```
Am obigen Beispiel kann man schön erkennen, dass es Sinn macht, die Verwendung der Anführungszeichen nach einem Schema vorzunehmen.
Es ist ratsam sich für eine Art von Anführungszeichen zu entscheiden, es hält alles übersichtlich.

## Datenimport

Der Datenimport stellt häufig eine Hürde dar, die Messdaten liegen ja teilweise gar nicht als tibble oder Vektor oder Liste in der R-Umgebung rum sondern häufig als Excel-Tabelle im Ordner Downloads oder auf dem Desktop, weit weg von unserer R-Umgebung, die R-Umgebung ist übrigens der virtuelle Taschenrechner.
Um nun Messdaten für uns nutzbar zu machen, müssen wir sie importieren wobei es nun wichtig ist zu wissen:

* wo die Daten sind
* in welchem Format sie vorliegen
* Was wir mit ihnen machen wollen

Der letze Punkt bezieht sich auf den Arbeitsaufwand.
Beispielhaft hat man vier bereits vorbearbeitete Messwerte, diese können auch skrupellos abgetippt oder kopiert werden.
Sollte es sich lohnen die Daten zu importieren stellt sich die Frage was eigentlich importiert wird, zum Beispiel eine Exceldatei oder eine CSV-Datei (comma separated value).
Das Dateiformat ist Maßgeblich relevant bei der Wahl einer Funktion zum Import.
Das "Wo" bezieht sich auf einen Dateipfad.
Dieser gibt uns an wo auf unserem System die Datei auffindbar ist.
Eine solche Datei muss aber nicht zwingend auf unserem Computer sein, sie kann sich auch im www befinden oder auf einem USB-Stick.
Hier ist die Verwendung von R Studio sehr bequem, es ermöglicht uns innerhalb unseres Computers Dateien zu Suchen und generiert dann einen Code um den Datensatz zu importieren.
Ein solcher Code besteht immer aus einer Import-Funktion, dem Dateipfad und optionalen Informationen:
```{r}
 # Laden der benötigten Funktion
library(readxl)
 # Anwenden der Funktion
Rohdaten <- read_excel("~/Bachelor/Paket_BHT/Methoden_generell/aerober_abbau_detergenzien.xlsx")
print(Rohdaten)
```
Der Importierte Datensatz heißt aerober_abbau_detergenzien.xlsx und wurde über readxl() importiert, in der R Umgebung heißt er nun Rohdaten.
Der Dateipfad ist in Anführungszeichen.
Nun liegt der Datensatz als "Rohdaten" vor und man kann damit arbeiten.

> es ist empfehlenswert die Messwerterfassung so einfach wie möglich durchzuführen, komplexe Tabellenstrukturen schaden da dem Datenimport und sind auch fürs Verständnis nicht unbedingt hilfreich.

Wenn beim Datenimport Probleme entstehen wird ein kleiner Text auf der Konsole erscheinen, dieser meistens sehr hilfreich um das Problem zu lösen, bei Härtefällen sollte eine Internetrecherche aber auch gute Lösungsansätze bringen.

## Einfache Operationen 

Die eben geladenen Messdaten sind nun innerhalb der R Umgebung für uns zugänglich, wir können sie nun untersuchen.
Im Beispieldatensatz wurde die $O_2$-Konzentration in Wasser vor und nach der Behandlung mit Mikroorganismen gemessen.
Das Wasser selbst wurde mit organischen Verschmutzungen belastet welche im Laufe der Zeit von Mikroorganismen abgebaut wurden.
Während des Abbaus wurde Sauerstoff verbraucht.
Die $O_2$-Anfangskonzentration sollte bei allen Gruppen gleich sein, wir können ja mal den Mittelwert berechnen: 
```{r}
# das Arithmetische Mittel:
mean(Rohdaten$`O2 [mg/L] davor`)
# und einen Boxplot:
boxplot(Rohdaten$`O2 [mg/L] davor`)
```

Bei genauerer Betrachtung fällt beim Boxplot ein Kreis auf Höhe der Null auf, es handelt sich um einen Datenpunkt welcher doch merkwürdig scheint.
Wir möchten diesen Wert genauer untersuchen, dazu können wir ein Zusammenfassung erstellen:
```{r}
summary(Rohdaten$`O2 [mg/L] davor`)
```

Wir sehen, dass das Minimum Null beträgt, eine recht unwahrscheinliches Ergebnis da im Versuch für alle sechs Durchläufe immer das Selbe Wasser aus dem selben Ansatz verwendet wurde.
Da keine Information zum Messwert vorliegt, welche explizit auf einen Messfehler verweist, sollten nun Überlegungen angestellt werden, ob es legitim ist den Wert als Ausreißer zu verwerfen.
Es gibt eine umfassendes Paket \texttt{outliers}:
```{r}
library(outliers) # laden des Paketes

# der Dixon-Test:

dixon.test(Rohdaten$`O2 [mg/L] davor`)

# Grubbs-Test:

grubbs.test(Rohdaten$`O2 [mg/L] davor`)

# Chi-im-quadrat-Test:

chisq.out.test(Rohdaten$`O2 [mg/L] davor`)

# entfernen des Ausreißer:

rm.outlier(Rohdaten$`O2 [mg/L] davor`) %>%
    boxplot()

```

Die Ausreißer-Tests geben immer eine kleine Zusammenfassung und eine Begründung zu ihrer Wahl, das erleichtert den Umgang mit ihnen.
Abhängig von der Fragestellung wird empfohlen sich mit der Arbeitsweise der Tests auseinander zu setzen, dann kann auch besser beurteilt werden, ob das gewählte Vorgehen zulässig ist.
 
## Datentransformation

Zu Beginn kann es oft schwer sein sich innerhalb von Datensätzen zurecht zu finden und jene Werte, welche von Interesse scheinen zugänglich zu machen.
Im obigen Beispiel hatten wir es leicht, es gab nur einen Ausreißer welcher sich leicht entfernen lies.
Um mit dem Datensatz nun weiter arbeiten zu können, möchten wir den Ausreißer aber ganz entfernen.
Das Paket \texttt{dplyr} bietet dazu die passenden Funktionen.
```{r}
# Filtern der sinvollen Werte
library(dplyr) # laden des benötigten Paket
Daten <- filter(Rohdaten
                , Rohdaten$"O2 [mg/L] davor" > 1) %>%
    print()
```

Wir sehen, dass wir nach der Datentransformation nur noch 7 Messungen in der Tabelle finden, wobei der Wert von Gruppe 3 fehlt, der Ausreißer
Das ging super mit \texttt{filter()}, zu Erst steht in den Klammern der Name des Datensatz, welcher bearbeitet werden soll und dann ein logisches Argument welches vorgibt, was geschehen soll.
In unserem Fall ein $ > 1$, also "Filter alle Werte, welche in der Spalte `O2 [mg/L] davor` einen Wert größer als eins aufweisen".

Die logischen Operatoren können sein:

\begin{description}
    \item[<, >] größer oder kleiner
    \item[<=, >=] größer gleich oder kleiner gleich
    \item[is.na()] also ist ein NA, "nicht auswertbar", Werte die fehlen
    \item[!] logische Verneinung 
    \item[!is.na()] ist KEIN NA
    \item[\&] und 
    \item[|] oder
    \item[\%in\%] erkennt ein definiertes Element
\end{description}

Operatoren können auch Außerhalb von Funktionen verwendet werden:

```{r}
Vektor.1 <- c(1, 2, 3, 4, 5)
# Wollen wir mal sehen, was %in% kann:

3 %in% Vektor.1

# man kann auch Vektoren in Vektoren suchen
c( 3, 4) %in% Vektor.1

# Was passiert, wenn ein Element nich vorkommt?
c(0, 22, 3) %in% Vektor.1
```

Mit \texttt{dplyr} besteht auch die Möglichkeit innerhalb des Datensatzes eine Funktion anzuwenden und mit ihr eine neue Spalte zu füllen, diese Funktion heißt \texttt{mutate}:

```{r}
# Wir berechnen die Differenz aus "davor" und "danach"
Daten <- mutate(Daten, Differenz = (`O2 [mg/L] davor` - `O2 [mg/L] danach` ))
print(Daten)
```


## Merkmale
Bei Studenten wird zur Qualitätsbewertung von Dozenten eigentlich immer nur zwischen "gut" oder "schlecht" unterscieden, so wie wir Farben als "grün" oder "rot" usw. bezeichnen, es gibt keine zwischengelagerten Zustände.
Sollte eine solche Einteilung vorgenommen werden, so liegt eine Nominalskala vor.
Das Gegenteil ist die Ordinalskala, sie kennt beliebig viele diskrete Zustände wie wir sie zum Beispiel am $pH$-Meter während einer Titration ablesen können.
Alternativ kann man natürlich auch beim Dozente-Ranking Punkte und sogar Punkte mit Nachkommastellen vergeben.
Abängig von diesen Merkmalen stehen uns verschiedene Möglichkeiten zur Verfügung, was im Anschluss mit unseren Messdaten geschieht und wie sie zu behandeln sind.
Die Intervallskala hingegen erlaubt es die Differenzen auf der Ordinalskala (Intervalle) miteinzubeziehen, also eine Merkmalsausprägung.
Zuletzt bleibt die Verhälnisskala, welche wie ihr Name verrät, Verhälnisse miteinbezieht und somit einen Vergleich unterschiedlicher Messungen erlaubt (*Covarianz* als Beispiel).

## Klassen und die Klassenbreite
In der Biotechnologie ist meist bekannt welcher Klasse eine Stichprobe angehört, die Klassen sind dabei die übergeordneten Gruppen welche wiederum Merkmalswerte enthalten welche in den einzelnen Stichproben wiederzufinden sind.
So als Beispiel eine Mehrfachbestimmung aus welcher dann eine Vielzahl technischer Replikate entsteht.
Bei einer Absorptionsspektroskopischen Untersuchung zum Beispiel mehrfach Messungen mit  gleichen Konzentrationen durchgeführt wobei immer Absorptionen gemessen werden, welche sich im Idealfall, kaum unterscheiden.
Die Klasse ist somit schon im Voraus klar.
Eine populäre Methode zur Berechnung der Klassenbreite $b$ ist Sturges-Regel [@doi:10.1080/01621459.1926.10502161] unter Berücksichtigung des Stichproben-Umfangs $n$ und der Spannweite $v$:
\[ b = \frac{v}{1 + 3.32 \cdot \log n } \approx \frac{v}{5 \log n} \]
oder nach Scott [@10.1093/biomet/66.3.605] unter Berücksichtigung der Standardabweichung $\sigma$ :
\[b = \frac {3{,}49 \cdot \sigma} {\sqrt[3]{n}}\]

## Mittelwert
Innerhalb einer solchen Klasse wird dann im eben beschriebenen Beispiel der Mittelwert errechnet.
In R geschieht dies über die Funktion *mean()*.
Eine Funktion kann aufgefasst werden als "Miniprogramm".
Damit dieses Miniprogramm läuft braucht es Informationen, diese bekommt es vom Anwender.
Diese Information ist, bezogen auf die Berechnung des arithmetischen Mittels, eine gewisse Anzahl von Werten, wobei für die Funktion *mean()* dabei schon alle wichtigen Information vorhanden sind, nämlich den Stichprobenumfang $n$ sowie die gemessenen Werte $x_i$:
\[\overline{n} = \frac{1}{n} \sum x_i\]
Hierzu verwenden wir die Funktion *c()*, sie fügt die einzelnen Werte zu einem Vektor zusammen.
```{r}
mean(c(1, 2, 3, 4))
# oder auch:
a <- c(1, 2.44, .56, 0.33, 1234)
mean(a)
```
**Wichtig ist zu beachten dass die Zahlen durch ein Komma getrennt werden und Nachkommastellen durch eine Punkt!**
Der Pfeil ($<-$) dient als Zeichen zur Zuordnung eines Objektes, wie unserem Vektor zu einer Variablen, unser a.
Eine Zuweisung in eine Andere Richtung ist unzulässig ($->$)
So können wir auch gleich eine Zuweisung des Mittelwert zu vornehmen:
```{r}
Mittelwert <- mean(a)
print(Mittelwert)
(Mittelwert)
```
Hier verwenden wir die Funktion *print()* um gleich den Mittelwert auf die Konsole gepromtet zu bekommen, das geschieht auch wenn wir die Variable *Mittelwert* umklammern.
So auch wenn wir eine ganze Rechenoperation umklammern:
```{r}
# Der Hashtag markiert Zeilen als Kommentar, alles was in der selben
# Zeile auf ihn folgt wird nicht mitinterpretiert
# runif verteitlt nun zehn Werte zufällig zwischen 20 und 200
(b <- runif(10, min = 20, max = 200))
(mean(b))
b <- sort(b) # Überschreiben des alten b zu einem neuen sortierten b
```
Die bisher gezeigten Funktionen gehören zu den Paketen {base} und {stats} und sind, im Gebrauch von R.Studio immer vorhanden, anders als Pakete welche explizit geladen werden müssen.
Auch zugänglich ist immer das Paket {graphics}:
```{r}
c <- runif(2222, 22, 100) # min und max kann man auch weglassen, ;-)
boxplot(b, c) # Ein einfacher Boxplot aus den Objekten a und b
hist(c) # Histogramm
nclass.Sturges(c) # Berechnung der Klassenbreite nach Sturges
nclass.scott(c) # Berechnung der Klassenbreite nach Scott
hist(c, 
     nclass = 13, # Wählen der Klassenbreite
     main = "Klassenbreite = 13" # einfügen einer Überschrift
     ) 
```

Das Paket {graphics} enthält eine umfassenden Sammlung an einfachen Plottingfunktionen, wer sich dafür interessiert ist unter https://www.rdocumentation.org/packages/graphics/versions/3.6.2 an der richtigen Stelle.


## Steuerungsparameter

Um beurteilen zu können ob ein Experiment Informationen liefert, die im Kontext zur Durchführung sinnvoll scheinen, können wir untersuchen wie die einzelnen Messwerte mit einander korrelieren.
Im Fall einer vierfach-Bestimmung ist das einfach nachvollziehbar:
```{r}
mean( c(1,2,3,4) )
mean( c(2,2.5,3,2.5))
```
Die technischen Replikate der ersten Messung liegen weit auseinander aber haben das selbe arithmetische Mittel wie die der zweiten Messung welche aber doch recht ähnliche Ergebnisse liefert!

Um einen Wert für die Abweichung der Messpunkte einer Messung zu einander zu können wir die  Summe der Abweichungsquadrate $SQ$ ermitteln:
\[SQ:=\sum\limits_{i=1}^n (x_i - \overline x)^2=(n-1) s_x^2\]
Wobei nun immer nur ein einzelner Punkt betrachtet wird, für die Messung selbst ist die Varianz $s_x^2$ von Interesse, sie beschreibt die Summe aller $SQ$:
\[s_{x}^{2}=\frac{1}{n-1} \sum \limits_{i=1}^n\left(x_i-\overline x\right)^2\]
Die dazugehörige Funktion in R heißt *var()*:
```{r}
var( c(1, 2, 3, 4))
var( c(2,2.5,3,2.5))
var( rep(2.5, 4)) # rep() lässt einen Vektor aus vier mal 2,5 entstehen!
```
Die Standardabweichung $s = \sqrt{s_x^2}$ wiederum ist die positive Wurzel der Varianz und bekommt die den Namen *sd()*:
```{r}
sqrt(var(c(1, 2, 3, 4))) # sqrt() ist die Quadratwurzel
var(c(1, 2, 3, 4)) ^ (1/2) # Wobei es auch so geht
sd( c(1, 2, 3, 4))
sd( c(2,2.5,3,2.5))
sd( rep(2.5, 4))
```
> R eignet sich im Labor super als Taschenrechner-Ersatz, Alle mathematischen Funktionen sind vorhanden, die Rechnungen können auch leicht modifiziert werden und wenn man sie als Skript schreibt kann super nachvollzogen werden was gerechnet wurde!
Dies hilft auch sich an die Sprache R zu gewöhnen, sollte aber kein allzu aufwendiger Prozess werden :-) (Ich hab's ja auch geschafft)

Möchten wir etwas mehr über unseren Mittelwert erfahren dividieren wir die Standard-Abweichung $s$ durch die Wurzel des Stichprobenumfang, also den Standardfehler $s_{\bar{x}}=\frac{s}{\sqrt{n}}$.
Wir haben nun alle Werkzeuge um die Funktion für den Standardfehler selbst zu definieren.
Dazu verwenden wir den Befehl *function()*:
```{r}
sde <- function(x) { # die Runde Klammer definiert das Argument der Funktion
  # Die geschweifte Klammer enthält die Definition der Funktion
  sd(x)/
    sqrt(length(x)) # length() ist die länge des Vektors von x
}
sde(c(1, 2, 3, 4))
sde(c(2,2.5,3,2.5))
sde(rep(2.5, 4))
```

Der Variationskoeffizient drückt prozentual die Abweichung der Standardabweichung zum arithmetischen Mittel aus $\left(cv=\frac{s}{\bar{x}}\cdot 100 \% \right)$.
```{r}
cv <- function(x){
return( (sd(x)/mean(x))*100 ) # return() lässt, wie die doppelte Umklammerung das Ergebnis sofort erscheinen
}
cv(c(1, 2, 3, 4))
cv(c(2,2.5,3,2.5))
cv(rep(2.5, 4))
```
Der Variationskoeffizient lässt zu, dass wir verschiedene Stichproben mit unterschiedlichen Mitteln vergleichen:
``` {r}
a <- c(500, 350, 400, 330, 370)
b <- c(50, 35, 40, 33, 37)
c <- c(234, 3, 44, 577, 9)
cv(a)
cv(b)
cv(c)
```

## Lineare Regression
Die lineare Regression ist eine recht häufig verwendete Methode in der BT, als klassisches Beispiel ist die Konzentrationsbestimmung über eine Regrssion ("Eichgerade", "Kalibrationsgerade" etc.) zu nennen.
Das Paket OlFaBA enthält die Funktion *plot_regression()* um schnell eine solche Regressionsgerade zu plotten:
```{r}
conc <- seq(0, 20, 5) 
abs <- runif(5, 0, 1) + conc # Simmulierte Absorptionsmessung
plot_regression(abs, conc)  # Plotten den Absorption gegen Konzentration
```

Dieser Plott zeigt uns, in grau hinterlegt die gleitenden durchnittliche Abweichung, auch ein Maß für die Zuverlässigkeit unserer Arbeitsweise, es könnt auch anders aussehen:
```{r}
conc <- seq(0, 20, 5) 
abs <- sort(runif(5, 0, 1)) + conc # Simmulierte Absorptionsmessung
plot_regression(abs, conc)  # Plotten den Absorption gegen Konzentration
```

Aber schlimmer geht ja bekanntlich immer: 
```{r}
# Erstellen einer Funktion:
LinMod_CEv <- function(abs, conc) {
  return(
  stats::lm(
    conc ~ abs
  )
)
  return(summary(LinMod_CEv))
}


conc <- seq(0, 20, 5) 
abs <- runif(5, 0, 5) + conc # Simmulierte Absorptionsmessung
plot_regression(abs, conc)  # Plotten den Absorption gegen Konzentration
Modell <- LinMod_CEv(abs, conc)
summary(Modell)
```

Das $R^2$ gibt uns bekanntlich die Genauigkeit unserer Regressionsgerade an, es wird als Bestimmtheitsmaß bezeichnet.
Alternativ wird im Laborjargon häufig auch der Begriff "Quadratfehler" verwendet. 
Um an diese Information zu kommen verwenden wir hierbei explizit eine Funktion aus dem Paket {base}, nämlich *summary()*, diese Funktion liefert uns eine Menge an Information, die wirklich relevanten sind hier zusammengestellt:
```{r echo=FALSE}
pander(summary(Modell))
```

**Intercept** ist der Schnittpunkt mit der $y$-Achse also $b$ und der Steigung $m$ als **conc** für $y = m x + b$.
Damit kann jetzt auch weitergearbeitet werden, nun kann die Konzentration einer unbekannten Probe über ihre Absorption bestimmt werden.
Dafür steht in {OlFaBA} die Funktion *conc_eval()* zur Verfügung, bevor wir diese jedoch verwenden muss *LinMod_CEv()* für die Standardreihe durchgeführt worden sein, im Beispiel mit realen Messdaten:
```{r}
conc <- seq(0, 100, length.out = 6) # Ein Vektor der Länge 6, die Konzentrationen den Standards
abs <- c(.374, .5, .617, .78, .874, .985) # Die Real gemessenen Konzentrationen 
conc_eval(abs_P = abs, abs_std = abs, conc_std = conc) #Berechnung der Konzentrationen der gemessenen Absorptionen aus dem Modell
```
Teil der Funktion conc_eval() ist es, die errechneten Daten in einer Tabelle darzustellen, diese kann besonders gut bei der Verwendung von R Markdown zur Geltung kommen.
```{r echo=FALSE}
KonzBer <- conc_eval(abs_P = abs, abs_std = abs, conc_std = conc)
daten <- tibble("Absorption"= abs, "Ist-Konzentration" = conc , "Berechnete Konzentration" = KonzBer)
pander(daten)
```

### Aufgaben

Bereche für eine realtive Absorption von $A_{595} = 0.33$ die Konzentration unter Berücksichtigung der Standardreihe:
```{r echo=FALSE}
tibble(
        "Absoption (bei 595 nm)" = c(0.13, 0.22, 0.30, 0.37, 0.44, 0.49, 0.54, 0.57),
        "Konzentration in µg/ml" = seq(0, 200, length.out = 8)
) %>%
pander()
```

## Nicht-lineare Regressionen
### Nicht-lineare Regression für die Michaelis-Menten-Funktion
Die Michaelis-Menten Gleichung kann mathematisch beschrieben werden als:
\[v_0 = \frac{v_\mathrm{max} \cdot [\mathrm S]}{K_{\mathrm m} + [\mathrm S]}\]
Wir können uns als nächstes wieder Test-Daten simulieren:
```{r}
sub <-seq(1,20,1)
velo <-(
  (runif(1,14.7,15)*sub
  )/( # Könnt ihr die MM-Kinetik wiedererkennen? ;-)
    runif(1,2.5,3)+sub))+rnorm(20,0,.3)
```
Im nächsten Schritt soll ein Fitting für die Substratvariation gegen die Enzymaktivität vorgenommen werden: 
```{r}
plot_MM_direct(sub, velo)
```

### Dose-Response-Modelle

Das Dose-Response re

* kompetitive Immunoessays
* LD-50 Bestimmung
* Ermittlung der wahrnehmbaren Konzentration eines Riechstoffes in der Umgebungsluft

Anhand von Daten eines kompetitiven ELISA's aus dem IC-Praktikum wirst du eine Anwendung sehen:
```{r}
conc <- c(0, .1, .2, .5, 1, 2, 5, 10, 20, 50) # Die Konzentration
resp <- c(1.014, 1.057, .996, .929, .856, .758, .586, .373, .284, .197) # die gemessene Absorption
plot(resp~log(conc)) # Ein einfacher Plot der Daten
dose_response_plot(conc, resp) # das eigentliche Modell
```

